# 数据的写入、更新、删除

> Elasticsearch采用多Shard方式，通过配置routing规则将数据分成多个数据子集。每个数据子集提供独立的索引和搜索功能。当写入文档的时候，根据routing规则，将文档发送到特定Shard中建立Segment。这样就能实现分布式了。

​		每个index由多个shard组成(默认5个)，每个Shard有一个主节点和多个replica。每次写入的时候，写入请求会先根据_routing规则选择发给哪个Shard，Index Request中可以设置使用哪个field作为路由参数。如果没有设置，则使用mapping中的配置，如果mapping中没有则使用id作为参数。

```
shard_num = hash(routing) % num_primary_shards
```

​		每一个shard由多个segment构成，每个segment都是一个倒排索引，再分片中进行搜索其实是在每个segment中进行搜索，然后将其结果合并，作为shard的返回结果。

​		通过routing的值选择出Shard，在集群中找到该shard的primary节点。请求发送到primary节点后执行完成后primary shard会将请求同时发送给多个Replica shard，请求在多个replica shard上执行成功返回primary shard后，写入请求执行成功，结果返回客户端。



## Elasticsearch写入过程

- 客户端选择一个 node 发送请求过去，这个 node 就是 `coordinating node`（协调节点）。
- `coordinating node` 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。
- 实际的 node 上的 `primary shard` 处理请求，然后将数据同步到 `replica node`。
- `coordinating node` 如果发现 `primary node` 和所有 `replica node` 都完成之后，就返回响应结果给客户端。



## Elasticsearch读取过程

可以通过 `doc id` 来查询，会根据 `doc id` 进行 hash，判断出来当时把 `doc id` 分配到了哪个 shard 上面去，从那个 shard 去查询。

- 客户端发送请求到任意一个 node，成为 `coordinate node`。
- `coordinate node` 对 `doc id` 进行哈希路由，将请求转发到对应的 node，此时会使用 `round-robin`随机轮询算法，在 `primary shard` 以及其所有 replica 中随机选择一个，让读请求负载均衡。
- 接收请求的 node 返回 document 给 `coordinate node`。
- `coordinate node` 返回 document 给客户端。



## Elasticsearch的写入底层原理

1.新增文档首先会被放在内存的缓存中。

2.当文档数量足够多或者达到一定时间窗口`refresh interval`，将把缓存中的文档写入segment，并打开segment使之可以被搜索到。(Refresh操作，默认每秒一次，elasticsearch称为实时搜索功能)

3.refresh行为会把缓存中的文档写入到segment中，但是此时创建的segment文件是写在文件系统的缓存中的。所以es会定期(30分钟)或者translog达到一定大小`index.translog.sync_interval`执行flush操作，将缓存中的segment全部写入磁盘并确保写入成功，同时创建commit point，完成完整的commit 过程。

![截屏2020-04-22下午3.12.56](/Users/denakira/Desktop/myworkspace/note/ES/picture/截屏2020-04-22下午3.12.56.png)



#### translog

​		前面流程中，如果缓存中的segment还没来得及被flush到磁盘时断电，也会造成数据丢失，因此es接受文档时，把文档放在buffer的同时，也会把文档记录在translog中。当所有缓存的segment写入磁盘后，translog才会被清空。



## Elasticsearch的更新和删除

​		segment是不能更改的，因此每个commit point都会维护一个.del文件，文件记录了在某个segment内某个文档已经被删除。在segment中，被删除的文档依旧是能够被搜索到的，不过在返回结果前，会根据.del把那些已经删除的文档从搜索结果中过滤掉。



## Segment的合并

​		定期接收到的文档将会被写入到新的segment中，这样经过一段时间就会有很多segment，造成资源的浪费。因此es会定期的将小的segment合并成大segment。

​		在 5.0 之前，归并线程的限速配置 `indices.store.throttle.max_bytes_per_sec` 是 20MB，在5.0之后，使用了Lucene的throttle机制不需要再手动配置参数，从代码中看10240MB。

#### 归并策略

- index.merge.policy.floor_segment 默认 2MB，小于这个大小的 segment，优先被归并。
- index.merge.policy.max_merge_at_once 默认一次最多归并 10 个 segment
- index.merge.policy.max_merge_at_once_explicit 默认 forcemerge 时一次最多归并 30 个 segment。
- index.merge.policy.max_merged_segment 默认 5 GB，大于这个大小的 segment，不用参与归并。forcemerge 除外。



## 参考资料

[](https://my.oschina.net/LucasZhu/blog/1542850)

[](https://www.cnblogs.com/bonelee/p/6617612.html)

[](https://blog.csdn.net/jiaojiao521765146514/article/details/83753215)

